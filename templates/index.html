<!doctype html>
<html>
<head>
  <meta charset="utf-8">
  <title>Speech to Text Demo</title>
  <style>
    body { font-family: Arial, sans-serif; background: #f5f5f5; }
    .container { max-width: 600px; margin: 40px auto; background: #fff; padding: 20px; border-radius: 8px; box-shadow: 0 0 10px rgba(0,0,0,0.1); }
    label { display: block; margin-top: 10px; }
    input[type="text"], select { width: 100%; padding: 8px; margin-top: 5px; box-sizing: border-box; }
    input[type="file"] { margin-top: 5px; }
    button { margin-top: 10px; padding: 10px 15px; }
    #result { white-space: pre-wrap; margin-top: 20px; background: #eee; padding: 10px; border-radius: 4px; }
  </style>
</head>
<body>
  <div class="container">
    <h1>Speech to Text Demo</h1>
    <form id="transcribe-form">
      <label>Base URL
        <input type="text" name="base_url" value="https://api.openai.com/v1">
      </label>
      <label>API Key
        <input type="text" name="api_key">
      </label>
      <label>Model
        <select name="model">
          <option value="gpt-4o-mini-transcribe">gpt-4o-mini-transcribe</option>
          <option value="gpt-4o-transcribe">gpt-4o-transcribe</option>
        </select>
      </label>
      <label>Audio File
        <input type="file" id="audio" name="audio">
      </label>
      <div>
        <button type="button" onclick="startRecording()">Start Recording</button>
        <button type="button" onclick="stopRecording()">Stop Recording</button>
        <button type="button" onclick="transcribe()">Transcribe</button>
      </div>
    </form>
    <p id="status"></p>
    <pre id="result"></pre>
  </div>

  <script>
    let mediaRecorder;
    let recordedChunks = [];

    function startRecording() {
      recordedChunks = [];
      navigator.mediaDevices.getUserMedia({ audio: true }).then(stream => {
        mediaRecorder = new MediaRecorder(stream);
        mediaRecorder.start();
        document.getElementById('status').innerText = 'Recording...';
        mediaRecorder.addEventListener('dataavailable', e => recordedChunks.push(e.data));
      }).catch(err => {
        document.getElementById('status').innerText = 'Microphone access denied.';
      });
    }

    function stopRecording() {
      if (mediaRecorder && mediaRecorder.state !== 'inactive') {
        mediaRecorder.stop();
        document.getElementById('status').innerText = 'Recording stopped.';
      }
    }

    function transcribe() {
      const form = document.getElementById('transcribe-form');
      const formData = new FormData(form);
      if (recordedChunks.length > 0) {
        const blob = new Blob(recordedChunks, { type: 'audio/webm' });
        formData.set('audio', blob, 'recording.webm');
      } else if (!document.getElementById('audio').files[0]) {
        alert('Please record or upload an audio file.');
        return;
      }
      fetch('/transcribe', { method: 'POST', body: formData })
        .then(res => res.json())
        .then(data => {
          document.getElementById('result').innerText = data.text || JSON.stringify(data);
        })
        .catch(err => {
          document.getElementById('result').innerText = 'Error: ' + err;
        });
    }
  </script>
</body>
</html>
